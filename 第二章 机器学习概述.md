# 第二章 机器学习概述

**机器学习（Machine Learning，ML）** 就是让计算机从数据中进行自动学习，得到某种知识或规律。

## 2.1 基本概念

1. 样本（示例）：特征＋标签
2. 数据集：训练集＋测试集
3. 学习（训练）：给定一组训练集$\mathcal{D}$，通过某种学习算法$\mathcal{A}$，我们可以从函数集合$\mathcal{F}$中学习到一个最优的函数，然后可以用这个函数预测标签的值或标签的条件概率。

## 2.2 机器学习的三个基本要素

### 2.2.1 模型

输入空间$\mathcal{X}$和输出空间$\mathcal{Y}$构成了一个样本空间，对于样本空间的样本$(x,y)\in\mathcal{X}\times\mathcal{Y}$，我们希望找到一个未知的真实映射函数$y = f(x)$或真实条件概率分布$p_r(y|x)$来描述，机器学习的目标就是找到这样一个模型来逼近真实映射函数$y = f(x)$或真实条件概率分布$p_r(y|x)$。

我们可以根据经验来设置一个函数集合$\mathcal{F}$，

$$\mathcal{F} = \{f(x;\theta)|\theta\in\mathbb{R}^D\}。$$

我们把$\mathcal{F}$称作**假设空间（Hypothesis Space）**，$\theta$为模型的参数。

常见的假设空间可以分为线性和非线性两种，对应的模型也分别称为线性模型和非线性模型。

#### 2.2.1.1 线性模型

$$f(x;\theta) = w^Tx + b$$

#### 2.2.1.2 非线性模型

广义的非线性模型可以写为多个非线性基函数$\phi(x)$的线性组合

$$f(x;\theta) = w^T\phi(x) + b，$$

其中$\phi(x) = [\phi_1(x), \ldots, \phi_K(x)]^T$为$K$个非线性基函数组成的向量。

如果$\phi(x)$本身为可学习的基函数，比如

$$\phi_k(x) = h(w^T\phi'(x)+b_k)，\forall1\le k \le K，$$

其中$h(\cdot)$为非线性函数，$\phi'(x)$为另一组基函数，则$f(x;\theta)$就等价于神经网络模型。

### 2.2.2 学习准则

模型的好坏可以通过**期望风险（Expected Risk）** 来衡量，

$$\mathcal{R}(\theta) = \mathbb{E}_{(x,y)\sim p_r(x,y)}[\mathcal{L}(y,f(x;\theta)]。$$

#### 2.2.2.1 损失函数

1. 0-1损失函数
2. 平方损失函数
3. 交叉熵损失函数
4. Hinge损失函数

#### 2.2.2.2 风险最小化准则

一个好的模型应该有一个比较小的期望风险，但由于我们无法知道真实的数据分布，期望风险是无法计算的。这里我们给定一个训练集$\mathcal{D} = \{(x^{(n)},y^{(n)})\}_{n=1}^N$，我们可以计算**经验风险（Empirical Risk）**，即在训练集上的平均损失

$$\mathcal{R}_{\mathcal{D}}^{emp}(\theta) = \frac{1}{N}\sum_{i=1}^N\mathcal{L}(y^{(n)},f(x^{(n)};\theta)。$$

我们需要找到一组参数使得经验风险最小，这就是**经验风险最小化（Empirical Risk Minimization，ERM）** 准则。

由大数定理，当训练集大小$\vert \mathcal{D}\vert$趋向于无穷大，经验风险就趋向于期望风险。但在现实中，我们只有有限个样本，并且训练样本往往是真实数据的一个很小的子集或者包含一定的噪声数据，不能很好反应全部数据的真实分布，因此追求经验风险最小化原则容易导致模型在训练集上错误率很低，但在未知数据上错误率很高，这就是所谓的**过拟合（overfitting）**。
