# 第二章 机器学习概述

**机器学习（Machine Learning，ML）** 就是让计算机从数据中进行自动学习，得到某种知识或规律。

## 2.1 基本概念

1. 样本（示例）：特征＋标签
2. 数据集：训练集＋测试集
3. 学习（训练）：给定一组训练集$\mathcal{D}$，通过某种学习算法$\mathcal{A}$，我们可以从函数集合$\mathcal{F}$中学习到一个最优的函数，然后可以用这个函数预测标签的值或标签的条件概率。

## 2.2 机器学习的三个基本要素

### 2.2.1 模型

输入空间$\mathcal{X}$和输出空间$\mathcal{Y}$构成了一个样本空间，对于样本空间的样本$(x,y)\in\mathcal{X}\times\mathcal{Y}$，我们希望找到一个未知的真实映射函数$y = f(x)$或真实条件概率分布$p_r(y|x)$来描述，机器学习的目标就是找到这样一个模型来逼近真实映射函数$y = f(x)$或真实条件概率分布$p_r(y|x)$。

我们可以根据经验来设置一个函数集合$\mathcal{F}$，

$$\mathcal{F} = \{f(x;\theta)|\theta\in\mathbb{R}^D\}。$$

我们把$\mathcal{F}$称作**假设空间（Hypothesis Space）**，$\theta$为模型的参数。

常见的假设空间可以分为线性和非线性两种，对应的模型也分别称为线性模型和非线性模型。

#### 2.2.1.1 线性模型

$$f(x;\theta) = w^Tx + b$$

#### 2.2.1.2 非线性模型

广义的非线性模型可以写为多个非线性基函数$\phi(x)$的线性组合

$$f(x;\theta) = w^T\phi(x) + b，$$

其中$\phi(x) = [\phi_1(x), \ldots, \phi_K(x)]^T$为$K$个非线性基函数组成的向量。

如果$\phi(x)$本身为可学习的基函数，比如

$$\phi_k(x) = h(w^T\phi'(x)+b_k)，\forall1\le k \le K，$$

其中$h(\cdot)$为非线性函数，$\phi'(x)$为另一组基函数，则$f(x;\theta)$就等价于神经网络模型。

### 2.2.2 学习准则

模型的好坏可以通过**期望风险（Expected Risk）** 来衡量，

$$\mathcal{R}(\theta) = \mathbb{E}_{(x,y)\sim p_r(x,y)}[\mathcal{L}(y,f(x;\theta)]。$$

#### 2.2.2.1 损失函数

1. 0-1损失函数
2. 平方损失函数
3. 交叉熵损失函数
4. Hinge损失函数

#### 2.2.2.2 风险最小化准则

一个好的模型应该有一个比较小的期望风险，但由于我们无法知道真实的数据分布，期望风险是无法计算的。这里我们给定一个训练集$\mathcal{D} = \{(x^{(n)},y^{(n)})\}_{n=1}^N$，我们可以计算**经验风险（Empirical Risk）**，即在训练集上的平均损失

$$\mathcal{R}_{\mathcal{D}}^{emp}(\theta) = \frac{1}{N}\sum_{i=1}^N\mathcal{L}(y^{(n)},f(x^{(n)};\theta))。$$

我们需要找到一组参数使得经验风险最小，这就是**经验风险最小化（Empirical Risk Minimization，ERM）** 准则。

由大数定理，当训练集大小$\vert \mathcal{D}\vert$趋向于无穷大，经验风险就趋向于期望风险。但在现实中，我们只有有限个样本，并且训练样本往往是真实数据的一个很小的子集或者包含一定的噪声数据，不能很好反应全部数据的真实分布，因此追求经验风险最小化原则容易导致模型在训练集上错误率很低，但在未知数据上错误率很高，这就是所谓的**过拟合（Overfitting）**。

> 定义-过拟合：给定一个假设空间$\mathcal{F}$，一个假设$f$属于$\mathcal{F}$，如果存在其他的假设$f'$也属于$\mathcal{F}$，使得在训练集上$f$的损失比$f'$的损失小，但在整个样本空间上$f'$的损失比$f$的损失小，那么就说假设$f$过度拟合训练数据。

过拟合问题的原因：
1. 训练数据少
2. 噪声
3. 模型能力强

为解决过拟合问题，一般在经验风险最小化的基础上引入参数的**正则化（Regulazation）** 来限制模型能力，使其不要过度地最小化经验风险。这种准则就是**结构风险最小化（Structure Risk Minimization，SRM）** 准则：

$$
\begin{align*}
    \theta^* &= \argmin_{\theta}\mathcal{R}_{\mathcal{D}}^{struct}(\theta) \\ &= \argmin_{\theta}\mathcal{R}_{\mathcal{D}}^{emp}(\theta) + \frac{1}{2}\lambda\Vert\theta\Vert^2 \\ &= \argmin_{\theta}\frac{1}{N}\sum_{i=1}^N\mathcal{L}(y^{(n)},f(x^{(n)};\theta)) + \frac{1}{2}\lambda\Vert\theta\Vert^2,
\end{align*}
$$

其中$\Vert\theta\Vert$是$\ell_2$范数的**正则化项**，用来减少参数空间，避免过拟合，$\lambda$用来控制正则化的强度。正则化项也可以使用其他函数，如$\ell_1$范数，$\ell_1$范数的引入通常会使得参数有一定稀疏性，因此在很多算法中也经常使用。从贝叶斯学习的角度来讲，正则化是引入了参数的先验分布，使其不完全依赖训练数据。

和过拟合相反的一个概念是**欠拟合（Underfitting）**，即模型不能很好地拟合训练数据，在训练集上错误率较高。欠拟合一般是由于模型的拟合能力不足造成的。

机器学习的学习准则不仅仅是拟合训练集上的数据，同时也要使得泛化错误最低，使得模型能够对未知的样本进行预测。因此，机器学习可以被看作一个从有限、高维、有噪声的数据上得到更一般性的泛化规律。

### 2.2.3 优化算法

在确定了训练集$\mathcal{D}$、假设空间$\mathcal{F}$以及学习准则后，如何找到一个最优的模型$f(x;\theta^*)$就成了一个最优化问题。机器学习的训练过程就是最优化问题的求解过程。

在机器学习中，优化又可以分为参数优化和超参数优化．模型$f(x;\theta)$中的$\theta$称为模型的参数，可以通过优化算法进行学习。除了可学习的参数$\theta$之外，还有一类参数是用来定义模型结构或优化策略的，这类参数叫作**超参数（Hyper-Parameter）**。常见的超参数包括：聚类算法中的类别个数、梯度下降法中的步长、正则化项的系数、神经网络的层数、支持向量机中的核函数等。超参数的选取一般都是组合优化问题，很难通过优化算法来自动学习。因此，超参数优化是机器学习的
一个经验性很强的技术，通常是按照人的经验设定，或者通过搜索的方法对一组超参数组合进行不断试错调整。

#### 2.2.3.1 梯度下降法

#### 2.2.3.2 提前停止

#### 2.2.3.3 随机梯度下降法

#### 2.2.3.4 小批量梯度下降法
